{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW1BQ/9uAUBN4MhSami6NA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinith-09/DL-lstsm/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHxaRJbZdmTQ",
        "outputId": "056951df-5bb3-4d98-9083-93ab9189e0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 71s 434ms/step - loss: 0.6605 - accuracy: 0.5798 - val_loss: 0.5239 - val_accuracy: 0.7568\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 64s 409ms/step - loss: 0.5041 - accuracy: 0.7520 - val_loss: 0.4660 - val_accuracy: 0.7810\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 64s 410ms/step - loss: 0.4745 - accuracy: 0.7761 - val_loss: 0.6240 - val_accuracy: 0.6438\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 64s 406ms/step - loss: 0.4903 - accuracy: 0.7624 - val_loss: 0.5496 - val_accuracy: 0.7264\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 63s 404ms/step - loss: 0.4303 - accuracy: 0.8080 - val_loss: 0.5271 - val_accuracy: 0.7376\n",
            "196/196 [==============================] - 22s 114ms/step - loss: 0.5268 - accuracy: 0.7342\n",
            "Test loss: 0.5268375873565674\n",
            "Test accuracy: 0.7341600060462952\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense,LSTM\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Set the maximum number of words to consider in the reviews\n",
        "max_words = 5000\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "# Pad the sequences to have the same length\n",
        "max_length = 500\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "# Build the GRU-based model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 100, input_length=max_length))\n",
        "model.add(layers.SimpleRNN(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs)"
      ],
      "metadata": {
        "id": "zUCY2ZEVdyPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea64cfaf-fc50-4499-b6c0-85374f678455"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 73s 371ms/step - loss: 0.3846 - accuracy: 0.8340\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 73s 372ms/step - loss: 0.3987 - accuracy: 0.8245\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 73s 370ms/step - loss: 0.3537 - accuracy: 0.8532\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 73s 371ms/step - loss: 0.3188 - accuracy: 0.8727\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 72s 370ms/step - loss: 0.3098 - accuracy: 0.8724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss1 = history.history['loss']\n",
        "train_acc1 = history.history['accuracy']\n",
        "xc1=range(len(train_acc1))"
      ],
      "metadata": {
        "id": "wx8haToWd1oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 100, input_length=max_length))\n",
        "model.add(layers.GRU(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "hist=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "HCK9bVL4d3_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79372ae-bea4-43f9-b38d-a08e3b85fa24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 136s 853ms/step - loss: 0.4922 - accuracy: 0.7436 - val_loss: 0.3894 - val_accuracy: 0.8306\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 133s 850ms/step - loss: 0.3014 - accuracy: 0.8770 - val_loss: 0.4193 - val_accuracy: 0.8546\n",
            "Epoch 3/5\n",
            " 80/157 [==============>...............] - ETA: 1:01 - loss: 0.2429 - accuracy: 0.9024"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss2 = hist.history['loss']\n",
        "train_acc2 = hist.history['accuracy']\n",
        "xc2=range(len(train_acc2))"
      ],
      "metadata": {
        "id": "zIJwalxFd6Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 100, input_length=max_length))\n",
        "model.add(layers.LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "h=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "b0RTPiCjd8t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss3 = h.history['loss']\n",
        "train_acc3 = h.history['accuracy']\n",
        "xc3=range(len(train_loss3))"
      ],
      "metadata": {
        "id": "6RgmBtzCd_Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(xc1, train_acc1)\n",
        "plt.plot(xc2, train_acc2)\n",
        "plt.plot(xc3, train_acc3)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"RNN\",\"GRU\",\"LSTM\"])"
      ],
      "metadata": {
        "id": "F5ddQKkZeCl1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}